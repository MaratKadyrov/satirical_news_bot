{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation with **GPT**\n",
        "Модель: sberbank-ai/rugpt3medium_based_on_gpt2"
      ],
      "metadata": {
        "id": "I7BiXqnlsbiq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6K5MjhAVMgc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzYmWnr6T7bP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import transformers\n",
        "\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "from transformers import GPT2LMHeadModel, AdamW\n",
        "\n",
        "\n",
        "PATH_TEXT = 'panorama.txt'\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw9AvTZSV77x"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BXehCw9WT47"
      },
      "source": [
        "with open(PATH_TEXT, encoding='utf8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = re.sub('\\n{2,}', '\\n', text)\n",
        "print(text[:400])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMqw_Eg7Xc-7"
      },
      "source": [
        "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "tokens = np.array(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pjq3dfEWM9D"
      },
      "source": [
        "l = len(tokens)//15\n",
        "train = []\n",
        "test = []\n",
        "for i in range(15):\n",
        "    if i%5 > 0:\n",
        "        train.extend(tokens[i*l: (i+1)*l])\n",
        "    else:\n",
        "        test.extend(tokens[i*l: (i+1)*l])\n",
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "\n",
        "print(len(tokens), len(train), len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUice-jgUpen"
      },
      "source": [
        "# download rugpt3small_based_on_gpt2 model ~526 mb\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    'sberbank-ai/rugpt3small_based_on_gpt2',\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgO-ioP9tm_R"
      },
      "source": [
        "batch_size = 8\n",
        "max_len = 256\n",
        "epochs = 5\n",
        "\n",
        "n_train = len(train)//(batch_size*max_len)\n",
        "n_test = len(test)//(batch_size*max_len)\n",
        "print(n_train, n_test)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
        "\n",
        "total_steps = n_train * epochs\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "def accuracy(y_true, logits):\n",
        "    return torch.mean((y_true[1:] == torch.argmax(logits, dim=2)[:-1]).float()).detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BE6pzkyV3H7"
      },
      "source": [
        "def prep_tensors(x, i, batch_size=batch_size, max_len=max_len):\n",
        "    batch_ids = x[i*batch_size*max_len: (i+1)*batch_size*max_len]\n",
        "    batch_ids = batch_ids.reshape(batch_size, max_len)\n",
        "    batch_ids = torch.tensor(batch_ids).to(device)\n",
        "    return batch_ids\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f'epoch {epoch}/{epochs} : training')\n",
        "\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    model.train()\n",
        "    pbar = tqdm(range(n_train))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(train, i)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss, logits, _ = model(batch_ids,\n",
        "                             token_type_ids=None, \n",
        "                            #  attention_mask=batch_mask,\n",
        "                             labels=batch_ids\n",
        "                             ).values()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(train_acc):.4f} loss {np.mean(train_loss):.4f}', refresh=True)\n",
        "\n",
        "    \n",
        "    print(f'epoch {epoch}/{epochs} : validation')\n",
        "    model.eval()\n",
        "    val_acc = []\n",
        "    val_loss = []\n",
        "    pbar = tqdm(range(n_test))\n",
        "    for i in pbar:\n",
        "        batch_ids = prep_tensors(test, i)\n",
        "        with torch.no_grad():        \n",
        "            loss, logits, _ = model(batch_ids, \n",
        "                                token_type_ids=None, \n",
        "                                # attention_mask=batch_mask,\n",
        "                                labels=batch_ids\n",
        "                                 ).values()\n",
        "        \n",
        "        val_loss.append(loss.item())\n",
        "        val_acc.append(accuracy(batch_ids, logits))\n",
        "        pbar.set_description(f'acc {np.mean(val_acc):.4f} loss {np.mean(val_loss):.4f}', refresh=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'model.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "Hq5BffyLRHNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, len_gen=20, temperature=1):\n",
        "    generated = tokenizer.encode(prompt)\n",
        "    context = torch.tensor([generated]).to(device)\n",
        "    past = None\n",
        "\n",
        "    for i in tqdm(range(len_gen)):\n",
        "        output, past = model(context, past_key_values=past).values()\n",
        "        output = output / temperature\n",
        "        token = torch.distributions.Categorical(logits=output[..., -1, :]).sample()\n",
        "        \n",
        "        generated += token.tolist()\n",
        "        context = token.unsqueeze(0)\n",
        "\n",
        "    sequence = tokenizer.decode(generated)\n",
        "\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "nw6Yf0VVTZfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPMNLEKlb28O"
      },
      "source": [
        "def text_generator(text):\n",
        "    prompt = text\n",
        "    prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    out = model.generate(\n",
        "        input_ids=prompt,\n",
        "        max_length=70,\n",
        "        num_beams=5,\n",
        "        do_sample=True,\n",
        "        temperature=3.,\n",
        "        top_k=50,\n",
        "        top_p=0.6,\n",
        "        no_repeat_ngram_size=4,\n",
        "        num_return_sequences=4,\n",
        "        ).cpu().numpy()\n",
        "    cut = textwrap.fill(tokenizer.decode(out[0]), 120).rfind('.') + 1\n",
        "    return textwrap.fill(tokenizer.decode(out[0]), 120)[:cut]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test text generation\n",
        "print(text_generator('Китай'))"
      ],
      "metadata": {
        "id": "w0_3TjLRRsRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Telegram bot on colab"
      ],
      "metadata": {
        "id": "toMTaMIcThsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install aiogram\n",
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "2YlNFs11dRsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from aiogram import Bot, types\n",
        "from aiogram.dispatcher import Dispatcher\n",
        "from aiogram.utils import executor\n",
        "from aiogram.types import ContentType, File, Message\n",
        "from bs4 import BeautifulSoup\n",
        "from subprocess import call\n",
        "import requests\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "TOKEN = \"your bot token\" # bot token (use @BotFather to get it)"
      ],
      "metadata": {
        "id": "WG1A1CutfWPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhy7kqrVvJam"
      },
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Инициализация бота\n",
        "bot = Bot(token=TOKEN)\n",
        "dp = Dispatcher(bot)\n",
        "\n",
        "\n",
        "@dp.message_handler(commands=['start'])\n",
        "async def start(message: types.Message):\n",
        "    user_name = message.from_user.full_name\n",
        "    user_id = message.from_user.id\n",
        "    await message.reply(f'Hello {user_name}! I generate sarcastic news, just start the phrase.')\n",
        "\n",
        "@dp.message_handler()\n",
        "async def echo(message: types.Message):\n",
        "    text = message.text\n",
        "    await message.reply(text_generator(text))\n",
        "\n",
        "# Start bot\n",
        "if __name__ == '__main__':\n",
        "    executor.start_polling(dp)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}